#!/usr/bin/env python3

import argparse
import json
from collections import defaultdict
from datetime import datetime

import pandas as pd

from .analyzer import RepoAnalyzer
from .github_utils import *
from .output_handler import OutputHandler

import logging

# logging ëª¨ë“ˆ ê¸°ë³¸ ì„¤ì • (analyzer.pyì™€ ë™ì¼í•œ ì„¤ì •)
logging.basicConfig(
    stream=sys.stdout,
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# í¬ë§· ìƒìˆ˜
FORMAT_TABLE = "table"
FORMAT_TEXT = "text"
FORMAT_CHART = "chart"
FORMAT_HTML = "html"
FORMAT_ALL = "all"

VALID_FORMATS = [FORMAT_TABLE, FORMAT_TEXT, FORMAT_CHART, FORMAT_HTML, FORMAT_ALL]
VALID_FORMATS_DISPLAY = ", ".join(VALID_FORMATS)

# ì¹œì ˆí•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•  ArgumentParser í´ë˜ìŠ¤
class FriendlyArgumentParser(argparse.ArgumentParser):
    def error(self, message: str) -> None:
        if '--format' in message:
            # --format ì˜µì…˜ì—ì„œë§Œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©ì ì •ì˜
            logger.error(f"âŒ ì¸ì ì˜¤ë¥˜: {message}")
            logger.error(f"ì‚¬ìš© ê°€ëŠ¥í•œ --format ê°’: {VALID_FORMATS_DISPLAY}")
        else:
            super().error(message)
        sys.exit(2)

def parse_arguments() -> argparse.Namespace:
    """ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = FriendlyArgumentParser(
        prog="python -m reposcore",
        usage=(
            "python -m reposcore [-h] [-v] [owner/repo ...] "
            "[--output dir_name] "
            f"[--format {{{VALID_FORMATS_DISPLAY}}}] "
            "[--check-limit] "
            "[--user-info path]"
        ),
        description="ì˜¤í”ˆ ì†ŒìŠ¤ ìˆ˜ì—…ìš© ë ˆí¬ì§€í† ë¦¬ì˜ ê¸°ì—¬ë„ë¥¼ ë¶„ì„í•˜ëŠ” CLI ë„êµ¬",
        add_help=False
    )
    # ì €ì¥ì†Œ ì¸ìë¥¼ í•˜ë‚˜ ì´ìƒ ë°›ë„ë¡ nargs="+"ë¡œ ë³€ê²½
    parser.add_argument(
        "repository",
        type=str,
        nargs="*",
        metavar="owner/repo",
        help="ë¶„ì„í•  GitHub ì €ì¥ì†Œë“¤ (í˜•ì‹: 'ì†Œìœ ì/ì €ì¥ì†Œ'). ì—¬ëŸ¬ ì €ì¥ì†Œì˜ ê²½ìš° ê³µë°± í˜¹ì€ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥"
    )
    parser.add_argument(
        "-h", "--help",
        action="help",
        help="ë„ì›€ë§ í‘œì‹œ í›„ ì¢…ë£Œ"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="ìì„¸í•œ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--output",
        type=str,
        default="results",
        metavar="dir_name",
        help="ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: 'results')"
    )
    parser.add_argument(
        "--format",
        choices=VALID_FORMATS,
        nargs='+',
        default=[FORMAT_ALL],
        metavar=f"{{{VALID_FORMATS_DISPLAY}}}",
        help =  f"ê²°ê³¼ ì¶œë ¥ í˜•ì‹ ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥, ì˜ˆ: --format {FORMAT_TABLE} {FORMAT_CHART}) (ê¸°ë³¸ê°’:'{FORMAT_ALL}')"
    )
    parser.add_argument(
        "--grade",
        action="store_true",
        help="ì°¨íŠ¸ì— ë“±ê¸‰ í‘œì‹œ"
    )
    parser.add_argument(
        "--use-cache",
        action="store_true",
        help="participants ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜¬ì§€ ì—¬ë¶€ (ê¸°ë³¸: APIë¥¼ í†µí•´ ìƒˆë¡œ ìˆ˜ì§‘)"
    )
    parser.add_argument(
        "--token",
        type=str,
        help="API ìš”ì²­ ì œí•œ í•´ì œë¥¼ ìœ„í•œ ê¹ƒí—ˆë¸Œ ê°œì¸ ì•¡ì„¸ìŠ¤ í† í° (í™˜ê²½ë³€ìˆ˜ GITHUB_TOKENìœ¼ë¡œë„ ì„¤ì • ê°€ëŠ¥)"
    )   
    parser.add_argument(
        "--check-limit",
        action="store_true",
        help="í˜„ì¬ GitHub API ìš”ì²­ ê°€ëŠ¥ íšŸìˆ˜ì™€ ì „ì²´ í•œë„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--user-info",
        type=str,
        help="ì‚¬ìš©ì ì •ë³´ íŒŒì¼ì˜ ê²½ë¡œ"
    )
    parser.add_argument(
        "--user",
        type=str,
        metavar="username",
        help="íŠ¹ì • ì‚¬ìš©ìì˜ ì ìˆ˜ì™€ ë“±ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤ (GitHub ì‚¬ìš©ìëª…)"
    )
    parser.add_argument(
        "--theme", "-t",
        choices=["default", "dark"],
        default="default",
        help="í…Œë§ˆ ì„ íƒ (default ë˜ëŠ” dark)"
    )
    parser.add_argument(
    "--weekly-chart",
    action="store_true",
    help="ì£¼ì°¨ë³„ PR/ì´ìŠˆ í™œë™ëŸ‰ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--semester-start",
        type=str,
        help="í•™ê¸° ì‹œì‘ì¼ (í˜•ì‹: YYYY-MM-DD, ì˜ˆ: 2025-03-04)"
    )
    parser.add_argument(
        "--min-contributions",
        type=int,
        default=1,
        help="ìµœì†Œ ê¸°ì—¬ ì ìˆ˜ê°€ ì§€ì • ê°’ ì´ìƒì¸ ì‚¬ìš©ìë§Œ ê²°ê³¼ì— í¬í•¨í•©ë‹ˆë‹¤.(ê¸°ë³¸ê°’ : 1)"
    )
    parser.add_argument(
    "--dry-run",
    action="store_true",
    help="ì‹¤ì œ ì‘ì—… ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ ì •ë³´ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤."
    )
    return parser.parse_args()

args = parse_arguments()

def handle_individual_user_mode(args):
    repo = args.repository[0]
    if not validate_repo_format(repo):
        logger.error(f"ì˜¤ë¥˜: ì €ì¥ì†Œ '{repo}'ëŠ” 'owner/repo' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ) 'oss2025hnu/reposcore-py'")
        sys.exit(1)
    if not check_github_repo_exists(repo):
        logger.warning(f"ì…ë ¥í•œ ì €ì¥ì†Œ '{repo}'ê°€ ê¹ƒí—ˆë¸Œì— ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.")
        sys.exit(1)
    analyzer = RepoAnalyzer(repo, theme=args.theme)
    analyzer.collect_PRs_and_issues()

    user_info = None
    if args.user_info and os.path.exists(args.user_info):
        with open(args.user_info, "r", encoding="utf-8") as f:
            user_info = json.load(f)

    repo_scores = analyzer.calculate_scores(user_info)
    user_lookup_name = user_info.get(args.user, args.user) if user_info else args.user

    if user_lookup_name in repo_scores:
        sorted_users = list(repo_scores.keys())
        rank = sorted_users.index(user_lookup_name) + 1
        score = repo_scores[user_lookup_name]["total"]
        print(f"[INFO] ì‚¬ìš©ì: {user_lookup_name}")
        print(f"[INFO] ì´ì : {score:.2f}ì ")
        print(f"[INFO] ë“±ìˆ˜: {rank}ë“± (ì „ì²´ {len(sorted_users)}ëª… ì¤‘)")
    else:
        print(f"[INFO] ì‚¬ìš©ì '{args.user}'ì˜ ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if args.user and len(args.repository) == 1:
    handle_individual_user_mode(args)
    sys.exit(0)

def merge_participants(
    overall: dict[str, dict[str, int]],
    new_data: dict[str, dict[str, int]]
) -> dict[str, dict[str, int]]:
    """ë‘ participants ë”•ì…”ë„ˆë¦¬ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""
    for user, activities in new_data.items():
        if user not in overall:
            overall[user] = activities.copy()
        else:
            # ê° í•­ëª©ë³„ë¡œ í™œë™ìˆ˜ë¥¼ ëˆ„ì í•©ì‚°í•©ë‹ˆë‹¤.
            for key, value in activities.items():
                overall[user][key] = overall[user].get(key, 0) + value
    return overall

def save_cli_args(args, output_dir="results"):
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, "settings.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(vars(args), f, indent=2, ensure_ascii=False)

def main() -> None:
    """Main execution function"""
    args = parse_arguments()
    save_cli_args(args, args.output)

    # repositoryê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
    if not args.repository:
        logger.error("âŒ ì €ì¥ì†Œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # í† í° ì²˜ë¦¬ ë‹¨ìˆœí™”
    if args.token:
        if args.token == '-':
            # í‘œì¤€ ì…ë ¥ì—ì„œ í† í° ì½ê¸°
            github_token = sys.stdin.readline().strip()
            os.environ['GITHUB_TOKEN'] = github_token
        else:
            # ëª…ë ¹í–‰ ì¸ìë¡œ ë°›ì€ í† í° ì„¤ì •
            os.environ['GITHUB_TOKEN'] = args.token
    
    # í† í° ê²€ì¦ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ì–´ì„œ)
    github_token = os.getenv('GITHUB_TOKEN')
    if github_token and len(github_token) > 0:
        validate_token()

    # --check-limit ì˜µì…˜ ì²˜ë¦¬: ì´ ì˜µì…˜ì´ ìˆìœ¼ë©´ repository ì¸ì ì—†ì´ ì‹¤í–‰ë¨.
    if args.check_limit:
        check_rate_limit() 
        sys.exit(0)

    # --user-info ì˜µì…˜ìœ¼ë¡œ ì§€ì •ëœ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€, JSON íŒŒì‹±ì´ ê°€ëŠ¥í•œì§€ ê²€ì¦
    if args.user_info:
        # 1) íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.isfile(args.user_info):
            logger.error("âŒ ì‚¬ìš©ì ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        # 2) JSON ë¬¸ë²• ì˜¤ë¥˜ í™•ì¸
        try:
            with open(args.user_info, "r", encoding="utf-8") as f:
                user_info = json.load(f)
        except json.JSONDecodeError:
            logger.error("âŒ ì‚¬ìš©ì ì •ë³´ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            sys.exit(1)
    else:
        user_info = None

    repositories: list[str] = args.repository
    # ì‰¼í‘œë¡œ ì—¬ëŸ¬ ì €ì¥ì†Œê°€ ì…ë ¥ëœ ê²½ìš° ë¶„ë¦¬
    final_repositories = list(dict.fromkeys(
        [r.strip() for repo in repositories for r in repo.split(",") if r.strip()]
    ))

    # ê° ì €ì¥ì†Œ ìœ íš¨ì„± ê²€ì‚¬ (ë¨¼ì € ë‹¤ ê²€ì‚¬)
    for repo in final_repositories:
        if not validate_repo_format(repo):
            logger.error(f"ì˜¤ë¥˜: ì €ì¥ì†Œ '{repo}'ëŠ” 'owner/repo' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ) 'oss2025hnu/reposcore-py'")
            sys.exit(1)
        if not check_github_repo_exists(repo):
            logger.warning(f"ì…ë ¥í•œ ì €ì¥ì†Œ '{repo}'ê°€ ê¹ƒí—ˆë¸Œì— ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.")
            sys.exit(1)

    logger.info(f"ì €ì¥ì†Œ ë¶„ì„ ì‹œì‘: {', '.join(final_repositories)}")

    overall_participants = {}
    all_repo_scores = {}
    all_repo_html_data = {}  # HTML ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì €ì¥

    #ì €ì¥ì†Œë³„ë¡œ ë¶„ì„ í›„ 'ê°œë³„ ê²°ê³¼'ë„ ì €ì¥í•˜ê¸°
    try:
        from tqdm import tqdm
    except ImportError:
        print("[ì˜¤ë¥˜] tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("pip install tqdm")
        exit(1)

    # í•™ê¸° ì‹œì‘ì¼ ë¯¸ë¦¬ ì²˜ë¦¬
    semester_start_date = None
    if args.weekly_chart:
        if not args.semester_start:
            logger.error("âŒ --weekly-chart ì‚¬ìš© ì‹œ --semester-start ë‚ ì§œë¥¼ ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            sys.exit(1)
        try:
            semester_start_date = datetime.strptime(args.semester_start, "%Y-%m-%d").date()
        except ValueError:
            logger.error("âŒ í•™ê¸° ì‹œì‘ì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            sys.exit(1)

    for repo in tqdm(final_repositories, desc="ì €ì¥ì†Œ ë¶„ì„ ì§„í–‰"):

        analyzer = RepoAnalyzer(repo, theme=args.theme)
        output_handler = OutputHandler(theme=args.theme)

        if args.weekly_chart:
            if not args.semester_start:
                logger.error("âŒ --weekly-chart ì‚¬ìš© ì‹œ --semester-start ë‚ ì§œë¥¼ ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
                sys.exit(1)
            try:
                semester_start_date = datetime.strptime(args.semester_start, "%Y-%m-%d").date()
                analyzer.set_semester_start_date(semester_start_date)
            except ValueError:
                logger.error("âŒ í•™ê¸° ì‹œì‘ì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                sys.exit(1)

        # ì €ì¥ì†Œë³„ ìºì‹œ íŒŒì¼ ìƒì„± (ì˜ˆ: cache_oss2025hnu_reposcore-py.json)
        cache_file_name = f"cache_{repo.replace('/', '_')}.json"
        cache_path = os.path.join(args.output, cache_file_name)

        os.makedirs(args.output, exist_ok=True)

        cache_update_required = os.path.exists(cache_path) and analyzer.is_cache_update_required(cache_path)

        if args.use_cache and os.path.exists(cache_path) and not cache_update_required:
            logger.info(f"âœ… ìºì‹œ íŒŒì¼({cache_file_name})ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ìºì‹œì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
            with open(cache_path, "r", encoding="utf-8") as f:
                cached_json = json.load(f)
                analyzer.participants = cached_json['participants']
                analyzer.previous_create_at = cached_json['update_time']
        else:
            if args.use_cache and cache_update_required:
                if args.verbose:
                    logger.info(f"ğŸ”„ ë¦¬í¬ì§€í† ë¦¬ì˜ ìµœê·¼ ì´ìŠˆ ìƒì„± ì‹œê°„ì´ ìºì‹œíŒŒì¼ì˜ ìƒì„± ì‹œê°„ë³´ë‹¤ ìµœê·¼ì…ë‹ˆë‹¤. GitHub APIë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            else:
                if args.verbose:
                    logger.info(f"ï¿½ï¿½ ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜ ìºì‹œ íŒŒì¼({cache_file_name})ì´ ì—†ìŠµë‹ˆë‹¤. GitHub APIë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            analyzer.collect_PRs_and_issues()
            if not getattr(analyzer, "_data_collected", True):
                logger.error("âŒ GitHub API ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
                logger.error("â„¹ï¸ ì¸ì¦ ì—†ì´ ì‹¤í–‰í•œ ê²½ìš° ìš”ì²­ íšŸìˆ˜ ì œí•œ(403)ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. --token ì˜µì…˜ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
                sys.exit(1)
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump({'update_time':analyzer.previous_create_at, 'participants': analyzer.participants, 'weekly_activity': dict(analyzer.weekly_activity)}, f, indent=2, ensure_ascii=False)

        try:
            # 1) ì‚¬ìš©ì ì •ë³´ ë¡œë“œ (ì—†ìœ¼ë©´ None)
            user_info = json.load(open(args.user_info, "r", encoding="utf-8")) \
                if args.user_info and os.path.exists(args.user_info) else None

            # ìŠ¤ì½”ì–´ ê³„ì‚°
            repo_scores = analyzer.calculate_scores(user_info, min_contributions=args.min_contributions)

            # --user ì˜µì…˜ì´ ì§€ì •ëœ ê²½ìš° ì‚¬ìš©ì ì ìˆ˜ ë° ë“±ìˆ˜ ì¶œë ¥
            user_lookup_name = user_info.get(args.user, args.user) if args.user and user_info else args.user
            if args.user and len(final_repositories) == 1 and user_lookup_name in repo_scores:
                sorted_users = list(repo_scores.keys())
                user_rank = sorted_users.index(user_lookup_name) + 1
                user_score = repo_scores[user_lookup_name]["total"]
                logger.info(f"[INFO] ì‚¬ìš©ì: {user_lookup_name}")
                logger.info(f"[INFO] ì´ì : {user_score:.2f}ì ")
                logger.info(f"[INFO] ë“±ìˆ˜: {user_rank}ë“± (ì „ì²´ {len(sorted_users)}ëª… ì¤‘)")
            elif args.user and len(final_repositories) == 1:
                logger.info(f"[INFO] ì‚¬ìš©ì '{args.user}'ì˜ ì ìˆ˜ê°€ ê³„ì‚°ëœ ê²°ê³¼ì— ì—†ìŠµë‹ˆë‹¤.")

            # ì¶œë ¥ í˜•ì‹
            formats = set(args.format)
            if FORMAT_ALL in formats:
                formats = {FORMAT_TABLE, FORMAT_TEXT, FORMAT_CHART, FORMAT_HTML}

            # dry-run option
            if args.dry_run:
                print(f"[DRY-RUN] ì €ì¥ì†Œ: {repo}")
                print(f"[DRY-RUN] ìºì‹œ ì‚¬ìš© ì—¬ë¶€: {'ì˜ˆ' if args.use_cache else 'ì•„ë‹ˆì˜¤'}")
                print(f"[DRY-RUN] API í˜¸ì¶œ ì—¬ë¶€: {'ì˜ˆì •' if not args.use_cache else 'ìŠ¤í‚µ ê°€ëŠ¥'}")
                print(f"[DRY-RUN] ìºì‹œ ê²½ë¡œ: {cache_path}")
                print(f"[DRY-RUN] ì˜ˆìƒ ì¶œë ¥ ë””ë ‰í† ë¦¬: {os.path.join(args.output, repo.replace('/', '_'))}")
                continue

            # ì €ì¥ì†Œë³„ í´ë” ìƒì„± (owner/repo -> owner_repo)
            repo_safe_name = repo.replace('/', '_')
            repo_output_dir = os.path.join(args.output, repo_safe_name)
            os.makedirs(repo_output_dir, exist_ok=True)
            all_repo_scores[repo_safe_name] = repo_scores

            results_saved = []

            # 1) CSV í…Œì´ë¸” ì €ì¥
            if FORMAT_TABLE in formats or FORMAT_HTML in formats:
                table_path = os.path.join(repo_output_dir, "score.csv")
                output_handler.generate_table(repo_scores, save_path=table_path)
                output_handler.generate_count_csv(repo_scores, save_path=table_path)
                if args.verbose:
                    logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {table_path}")
                results_saved.append("CSV")

            # 2) í…ìŠ¤íŠ¸ í…Œì´ë¸” ì €ì¥
            if FORMAT_TEXT in formats:
                txt_path = os.path.join(repo_output_dir, "score.txt")
                output_handler.generate_text(repo_scores, txt_path)
                if args.verbose:
                    logger.info(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {txt_path}")
                results_saved.append("TXT")

            # 3) ì°¨íŠ¸ ì´ë¯¸ì§€ ì €ì¥
            if FORMAT_CHART in formats or FORMAT_HTML in formats:
                chart_filename = "chart_grade.png" if args.grade else "chart.png"
                chart_path = os.path.join(repo_output_dir, chart_filename)
                output_handler.generate_chart(repo_scores, save_path=chart_path, show_grade=args.grade)
                if args.verbose:
                    logger.info(f"ì°¨íŠ¸ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {chart_path}")
                results_saved.append("Chart")

            # HTML ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (ë‚˜ì¤‘ì— í†µí•© HTML ìƒì„±ì„ ìœ„í•´)
            if FORMAT_HTML in formats:
                # ì°¨íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„
                chart_filename = "chart_grade.png" if args.grade else "chart.png"
                chart_path = os.path.join(repo_output_dir, chart_filename)
                
                # ì£¼ê°„ ì°¨íŠ¸ ê²½ë¡œ ì¤€ë¹„
                weekly_chart_path = os.path.join(repo_output_dir, "weekly_activity.png") if args.weekly_chart else ''
                
                # ì €ì¥ì†Œë³„ ë°ì´í„° ì €ì¥
                all_repo_html_data[repo_safe_name] = {
                    'scores': repo_scores,
                    'chart_path': chart_path,
                    'weekly_chart_path': weekly_chart_path if args.weekly_chart else ''
                }

            # ìµœì¢… í†µí•© ë¡œê·¸ ì¶œë ¥
            logger.info(f"{repo} ë¶„ì„ ê²°ê³¼({', '.join(results_saved)}) ì €ì¥ ì™„ë£Œ: {repo_output_dir}")
            
            # HTML ë³´ê³ ì„œëŠ” ëª¨ë“  ì €ì¥ì†Œ ì²˜ë¦¬ í›„ì— í•œ ë²ˆë§Œ ìƒì„±í•  ì˜ˆì •ì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ

            # ì£¼ì°¨ë³„ í™œë™ ì°¨íŠ¸ìƒì„±
            if args.weekly_chart:
                analyzer.set_semester_start_date(semester_start_date)
                weekly_chart_path = os.path.join(repo_output_dir, "weekly_activity.png")
                output_handler.generate_weekly_chart(analyzer.weekly_activity, semester_start_date, weekly_chart_path)

            # ì „ì²´ ì°¸ì—¬ì ë°ì´í„° ë³‘í•©
            overall_participants = merge_participants(overall_participants, analyzer.participants)

        except Exception as e:
            logger.error(f"âŒ ì €ì¥ì†Œ '{repo}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue

    # ì „ì²´ ì €ì¥ì†Œ í†µí•© ë¶„ì„
    if len(final_repositories) > 1:
        if args.weekly_chart:
            overall_weekly_activity = defaultdict(lambda: {"pr": 0, "issue": 0})
            for repo in final_repositories:
                logger.info(f"ë¶„ì„ ì‹œì‘: {repo}")

                analyzer = RepoAnalyzer(repo, theme=args.theme)
                if args.weekly_chart:
                    analyzer.set_semester_start_date(semester_start_date)

                cache_file = f"cache_{repo.replace('/', '_')}.json"
                cache_path = os.path.join(args.output, cache_file)
                if os.path.exists(cache_path):
                    with open(cache_path, "r", encoding="utf-8") as f:
                        cache_data = json.load(f)
                        repo_weekly = cache_data.get("weekly_activity", {})
                        for week_str, data in repo_weekly.items():
                            week = int(week_str)
                            overall_weekly_activity[week]["pr"] += data.get("pr", 0)
                            overall_weekly_activity[week]["issue"] += data.get("issue", 0)

            overall_output_dir = os.path.join(args.output, "overall")
            os.makedirs(overall_output_dir, exist_ok=True)

            weekly_chart_path = os.path.join(overall_output_dir, "weekly_activity.png")
            output_handler.generate_weekly_chart(overall_weekly_activity, semester_start_date, weekly_chart_path)

        logger.info("\n=== ì „ì²´ ì €ì¥ì†Œ í†µí•© ë¶„ì„ ===")

        # í†µí•© ë¶„ì„ì„ ìœ„í•œ analyzer ìƒì„±
        overall_analyzer = RepoAnalyzer("multiple_repos", theme=args.theme)
        overall_analyzer.participants = overall_participants

        # í†µí•© ì ìˆ˜ ê³„ì‚°
        overall_scores = overall_analyzer.calculate_scores(user_info, min_contributions=args.min_contributions)

        # ì €ì¥ì†Œë³„ ì‚¬ìš©ì ì ìˆ˜ í†µí•© ë°ì´í„°
        user_scores = defaultdict(dict)
        for repo_name, repo_scores in all_repo_scores.items():
            for username, score_dict in repo_scores.items():
                user_scores[username][repo_name] = score_dict["total"]
        for username in user_scores:
            user_scores[username]["total"] = sum(user_scores[username].values())

        # ì •ë ¬
        user_scores = defaultdict(dict, sorted(user_scores.items(), key=lambda x: x[1]['total'], reverse=True))
        # rank ì¶”ê°€
        current_rank = 1
        prev_score = None

        for i, (username, scores) in enumerate(user_scores.items()):
            current_score = scores['total']
            
            # ë™ì ì ì²˜ë¦¬
            if prev_score is not None and current_score != prev_score:
                current_rank = i + 1
            
            user_scores[username]['rank'] = current_rank
            prev_score = current_score

        # í†µí•© ê²°ê³¼ ì €ì¥
        overall_output_dir = os.path.join(args.output, "overall")
        os.makedirs(overall_output_dir, exist_ok=True)

        # ê²°ê³¼ë¥¼ HTML ë°ì´í„°ì— ì¶”ê°€ (ìˆœì„œ ìˆ˜ì •)
        if FORMAT_HTML in formats:
            all_repo_html_data["overall"] = {
                'scores': overall_scores,
                'chart_path': os.path.join(overall_output_dir, "ratio_chart.png")
            }
            all_repo_html_data["overall_repository"] = {
                'scores': user_scores,
                'chart_path': os.path.join(overall_output_dir, "overall_chart.png")
            }

        results_saved = []
        # CSV ì €ì¥
        if FORMAT_TABLE in formats or FORMAT_HTML in formats:
            table_path = os.path.join(overall_output_dir, "ratio_score.csv")
            output_handler.generate_table(overall_scores, save_path=table_path)
            output_handler.generate_count_csv(overall_scores, save_path=table_path)
            if args.verbose:
                logger.info(f"[í†µí•© ì €ì¥ì†Œ] CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {table_path}")
            results_saved.append("CSV")

        # í…ìŠ¤íŠ¸ ì €ì¥
        if FORMAT_TEXT in formats:
            txt_path = os.path.join(overall_output_dir, "ratio_score.txt")
            output_handler.generate_text(overall_scores, txt_path)
            if args.verbose:
                logger.info(f"[í†µí•© ì €ì¥ì†Œ] í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {txt_path}")
            results_saved.append("TXT")

        # ì°¨íŠ¸ ì´ë¯¸ì§€ ì €ì¥
        if FORMAT_CHART in formats or FORMAT_HTML in formats:
            chart_filename = "chart_grade.png" if args.grade else "ratio_chart.png"
            chart_path = os.path.join(overall_output_dir, chart_filename)
            output_handler.generate_chart(overall_scores, save_path=chart_path, show_grade=args.grade)
            if args.verbose:
                logger.info(f"[í†µí•© ì €ì¥ì†Œ] ì°¨íŠ¸ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {chart_path}")
            results_saved.append("Chart")

        logger.info(f"[í†µí•© ì €ì¥ì†Œ] ë¶„ì„ ê²°ê³¼({', '.join(results_saved)}) ì €ì¥ ì™„ë£Œ: {overall_output_dir}")


    # ì‚¬ìš©ìë³„ ì €ì¥ì†Œë³„ ì ìˆ˜ CSV ë§Œë“œëŠ” í•¨ìˆ˜
    def generate_overall_repository_csv(all_repo_scores, output_path):
        user_scores = defaultdict(dict)

        for repo_name, repo_scores in all_repo_scores.items():
            for username, score_dict in repo_scores.items():
                user_scores[username][repo_name] = score_dict["total"]

        for username in user_scores:
            user_scores[username]["total"] = sum(user_scores[username].values())

        df = pd.DataFrame.from_dict(user_scores, orient='index').fillna(0)
        df.index.name = "name"
        column_order = [
            "oss2025hnu_reposcore-py",
            "oss2025hnu_reposcore-js",
            "oss2025hnu_reposcore-cs",
            "total"
        ]
        existing_columns = [col for col in column_order if col in df.columns]
        df = df[existing_columns]
        df = df.astype(int)
        df.reset_index(inplace=True)
        df = df[["name"] + existing_columns]
        df['rank'] = df['total'].rank(method='min', ascending=False).astype(int)
        for _, row in df.iterrows():
            username = row['name']
            user_scores[username]['rank'] = int(row['rank'])
        df = df.sort_values(by='rank')
        cols = ['rank'] + [col for col in df.columns if col != 'rank']
        df = df[cols]
        df.to_csv(output_path, encoding="utf-8", index=False)
        return user_scores
    
    if len(final_repositories) > 1:
        # ì €ì¥ ê²½ë¡œ ì§€ì •í•˜ê³  ìƒì„±
        overall_repo_dir = os.path.join(args.output, "overall")

        results_saved = []

        overall_csv_path = os.path.join(overall_repo_dir, "overall_scores.csv")
        user_scores = generate_overall_repository_csv(all_repo_scores, overall_csv_path)
        if args.verbose:
            logger.info(f"[ğŸ“Š overall_repository] ì €ì¥ì†Œë³„ ì‚¬ìš©ì ì ìˆ˜ CSV ì €ì¥ ì™„ë£Œ: {overall_csv_path}")
        results_saved.append("CSV")

        # ğŸ”½ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥: overall_scores.txt
        from prettytable import PrettyTable

        overall_txt_path = os.path.join(overall_repo_dir, "overall_scores.txt")
        table = PrettyTable()
        table.field_names = ["Rank", "Name"] + [repo.replace("/", "_") for repo in final_repositories] + ["Total"]

        sorted_users = sorted(user_scores.items(), key=lambda x: x[1]["total"], reverse=True)

        for username, score_dict in sorted_users:
            row = [score_dict['rank'], username]
            for repo in final_repositories:
                repo_key = repo.replace("/", "_")
                row.append(score_dict.get(repo_key, 0))
            row.append(score_dict["total"])
            table.add_row(row)

        with open(overall_txt_path, "w", encoding="utf-8") as f:
            f.write(table.get_string())
        if args.verbose:
            logger.info(f"[ğŸ“Š overall_repository] ì €ì¥ì†Œë³„ ì‚¬ìš©ì ì ìˆ˜ TXT ì €ì¥ ì™„ë£Œ: {overall_txt_path}")
        results_saved.append("TXT")

        # ğŸ“ˆ í†µí•© ì°¨íŠ¸ ì´ë¯¸ì§€ ì €ì¥
        chart_path = os.path.join(overall_repo_dir, "overall_chart.png")
        output_handler.generate_repository_stacked_chart(user_scores, save_path=chart_path)
        if args.verbose:
            logger.info(f"[ğŸ“Š overall_repository] ëˆ„ì  ê¸°ì—¬ë„ ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: {chart_path}")
        results_saved.append("Chart")

        logger.info(f"[ğŸ“Š overall_repository] ë¶„ì„ ê²°ê³¼({', '.join(results_saved)}) ì €ì¥ ì™„ë£Œ: {overall_repo_dir}")
        logger.info(f"[ğŸ“Š overall_repository] í†µí•© ì €ì¥ì†Œ ê¸°ì¤€ ì‚¬ìš©ìë³„ ê¸°ì—¬ë„ëŠ” '{overall_repo_dir}' í´ë” ë‚´ ê²°ê³¼ íŒŒì¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # --user ì˜µì…˜ì´ ì§€ì •ëœ ê²½ìš° í†µí•© ì ìˆ˜ì—ì„œ ì¶œë ¥
        user_lookup_name = user_info.get(args.user, args.user) if args.user and user_info else args.user
        if args.user and user_lookup_name in overall_scores:
            sorted_users = list(overall_scores.keys())
            user_rank = sorted_users.index(user_lookup_name) + 1
            user_score = overall_scores[user_lookup_name]["total"]
            print()
            logger.info(f"[INFO] ì‚¬ìš©ì: {user_lookup_name}")
            logger.info(f"[INFO] ì´ì : {user_score:.2f}ì ")
            logger.info(f"[INFO] ë“±ìˆ˜: {user_rank}ë“± (ì „ì²´ {len(sorted_users)}ëª… ì¤‘)")
            print()
        elif args.user:
            logger.info(f"[INFO] ì‚¬ìš©ì '{args.user}'ì˜ ì ìˆ˜ê°€ í†µí•© ë¶„ì„ ê²°ê³¼ì— ì—†ìŠµë‹ˆë‹¤.")
    
    # HTML ë³´ê³ ì„œ ìƒì„± (ëª¨ë“  ì €ì¥ì†Œ ì²˜ë¦¬ í›„ í•œ ë²ˆë§Œ ì‹¤í–‰)
    if not args.dry_run and FORMAT_HTML in formats and all_repo_html_data:
        logger.info("HTML ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        output_handler.generate_html_report(all_repo_html_data, args.output)
        logger.info("HTML ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")



if __name__ == "__main__":
    main()
